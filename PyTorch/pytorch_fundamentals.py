# -*- coding: utf-8 -*-
"""pytorch_fundamentals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zKN-QguRkzqIy14fdn3MQ9erNs5Da194
"""

import torch
import pandas as pd
import numpy as pd
import matplotlib.pyplot as plt

print(torch.__version__)

"""### Scalar/Vector/MATRIX/TENSOR"""

vector = torch.tensor([7,7])
vector

vector.shape

MATRIX = torch.tensor([[7,6],
                       [8,9]])
MATRIX

MATRIX.ndim

MATRIX[1]

MATRIX.shape

MATRIX = torch.tensor([[7,6],
                       [8,9]])
MATRIX.shape

MATRIX.shape

TENSOR = torch.tensor([[[1,2,3],
                        [3,6,9],
                        [2,4,5]]])
TENSOR

TENSOR.ndim

TENSOR.shape

TENSOR[0]

"""### Random Tensors"""

# create random tensor of size (3, 4)
random_tensor = torch.rand(3, 4)
random_tensor

random_tensor.ndim

# create a random tensor with similar shape as an image tensor
random_image_size_tensor = torch.rand(size=(3, 224, 224)) # color channels (R, G, B), height, width,
random_image_size_tensor.shape, random_image_size_tensor.ndim

"""### Tensors of ones / zeros & Range"""

# Create a tensor of all zeros
zero = torch.zeros(3,4)
zero

ones = torch.ones(3,4)
ones

ones.dtype

one_to_ten = torch.arange(1, 11)
one_to_ten

# creating tensor-like
ten_zeros = torch.zeros_like(one_to_ten)
ten_zeros

"""### Tensor Data Types"""

# Float 32 tensor
float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                                dtype=None,           # Data type of tensor
                                device=None,          # Device that tensor on
                                requires_grad=False)  # Tracks the gradients of a tensor while going through numerical calculations
float_32_tensor

float_32_tensor.dtype

float_16_tensor = float_32_tensor.type(torch.half)
float_16_tensor

int_32_tensor = torch.tensor([3, 6, 9], dtype=torch.int32)
float_32_tensor * int_32_tensor

"""### Getting information from tensors

1. get datatype from tensor `tensor.type`
2. get shape from tensor `tensor.shape`
3. get device from a tensor `tensor.device`

"""

# Create a Tensor
some_tensor = torch.rand(3, 4)
some_tensor

print(some_tensor)
print(f"Datatype of tensor: {some_tensor.dtype}")
print(f"Shape of tensor: {some_tensor.shape}")
print(f"Device of tensor: {some_tensor.device}")

"""### Manipulating Tensor (operations)

"""

# Create a tensor and add 100
tensor = torch.tensor([1,2,3])
tensor + 100

# Tensor multiplied by 10
tensor * 10

# subtract 10
tensor - 10

# Using PyTorch functions
torch.mul(tensor, 10)

"""### Matrix Multiplication"""

# Element Wise Multiplication
print(tensor, "*", tensor)
print(f"Equals: {tensor * tensor}")
tensor

# Commented out IPython magic to ensure Python compatibility.
# # Matrix Multiplication
# %%time
# torch.matmul(torch.rand(50), torch.rand(50))

# Commented out IPython magic to ensure Python compatibility.
# # Matrix multiplication by hand
# %%time
# value = 0
# for i in range(len(tensor)):
#   value += torch.rand(50)[i] * torch.rand(50)[i]
# print(value)

# Other notation for matmul
tensor @ tensor

# Shapes for matrix multiplications
tensor_A = torch.tensor([[1, 2],
                        [3, 4],
                        [5, 6]])
tensor_B = torch.tensor([[7, 10],
                        [8, 11],
                        [9, 12]])
torch.mm(tensor_A, tensor_B.T)  # .T is transpose

"""### Tensor Aggregation"""

# Finding the min, max, mean, etc
x = torch.arange(0, 100, 10)
x

torch.min(x), x.min()

torch.max(x), x.max()

# Torch.mean requires float or complex dtype
torch.mean(x.type(torch.float32)), x.type(torch.float32).mean()

torch.sum(x), x.sum()

# returnsindex of min or max
torch.argmin(x), torch.argmax(x)

x.argmin(), x.argmax()

"""### Reshaping, stacking, squeezing and unsqueezing

* Reshaping - reshapes and input tensor to a defined shape
* View - return a view of an input tensor of a certain shape but keep the same memory of original tensor
* Stacking - stacks tensors V-stack - vertical, H-stack - horizontal
* Squeeze - removes all `1` dimensions from a tensor
* unsqueeze - adds `1` dimension to out target tensor
* permute - return a view of the input with dimensions swapped
"""

import torch

x = torch.arange(1, 10)
x, x.shape

# add extra dimension - must contain same amount of elements
x_reshape = x.reshape(1, 9)
x_reshape, x_reshape.shape

# change the view - shares same memory as the same input
z = x.view(1, 9)
z, z.shape

# changing z, changes x
z[:, 0] = 5
z, x

# stack tensors on top - can also use .vstack & .hstack
x_stacked = torch.stack([x, x, x, x], dim=0)
x_stacked

x_reshape.shape

x_reshape

# removes all single (1) dimensions
x_reshape.squeeze()

print(f"Previous tensor: {x_reshape}")
print(f"New tensor: {x_reshape.squeeze()}")

x_reshape.unsqueeze(dim=2).shape

from os import XATTR_SIZE_MAX
# torch.permute - returns a view of the original tensor input with its dimensions permuted
x_original = torch.rand(224, 224, 3)

x_original.shape
x_permuted = x_original.permute(2, 0, 1) # permutes axis
x_permuted.shape

"""### Indexing"""

x = torch.arange(1, 10).reshape(1, 3, 3)
x, x.shape

x[0, 0]

x[0][1][2]

x[0, :, 1]

x[:, 1, 1]

x[0, 0, :]

x[0, 2, 2]

x[0, :, 2]

"""### Tensors and NumPy

* `torch.from_numpy(nparray)` - takes data from NumPy array and converts to a tensor
* `torch.tensor.numpy()` - takes pytorch tensor to numpy array
"""

import torch
import numpy as np

array = np.arange(1., 8.)
tensor = torch.from_numpy(array)
array, tensor
# default numpy dtype is float64, pytorch reflects this, unless specified otherwise

array.dtype

array = array + 1
array, tensor

# from tensor to numpy
tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor

tensor = tensor + 1
tensor, numpy_tensor

"""### Reproducability

to reduce randomness in pytorch and neural networks -> `Random Seed`
"""

import torch

randomA = torch.rand(3, 4)
randomB = torch.rand(3, 4)

print(randomA)
print(randomB)
print(randomA == randomB)

# make random but reproducable tensors
# random seed (.manual_Seed()) adds "Flavour" or randomness

RANDOM_SEED = 42

torch.manual_seed(RANDOM_SEED)
randomC = torch.rand(3, 4)

torch.manual_seed(RANDOM_SEED)
randomD = torch.rand(3, 4)
print(randomC)
print(randomD)
print(randomC == randomD)

"""### Running tensors and pytorch objects on GPU (making faster computations)

* google colab
* purchase GPU
* cloud computing - GCP, AWS, Azure
"""

import torch
torch.cuda.is_available()

# Setup device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

# counts number of devices
torch.cuda.device_count()

"""## Putting tensors (and models) on the GPU"""

# Create a tensor
tensor = torch.tensor([1, 2, 3])

# tensor not on GPU
print(tensor, tensor.device)

# Move tensor to GPU if available
tensor_on_gpu = tensor.to(device)
tensor_on_gpu

# Move tensors back to CPU
# tensor_on_gpu.numpy() causes device ERROR, must set tp CPU

tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu